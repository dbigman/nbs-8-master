{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document and Documentation Search - Retrieval Step Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OPEN AI EMBEDDINGS:\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "API_KEY = \"\"\n",
    "\n",
    "# Create the embeddings function\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key = API_KEY)\n",
    "\n",
    "# create a text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "print('Cell finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the document and split it into chunks\n",
    "document_dir = \"./\"\n",
    "filename = \"powerbi_book.pdf\"\n",
    "file_path = os.path.join(document_dir, filename)\n",
    "\n",
    "pages = PyPDFLoader(file_path).load_and_split()\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
    "print('Cell finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[0].page_content, '\\n\\n')\n",
    "\n",
    "data = db._collection.get(include=['embeddings'])\n",
    "print(data['embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query it\n",
    "\n",
    "#user_question = 'How do I build one of those charts that look like a Swiss Cheese?'\n",
    "#user_question = 'Can I onboard data using SQL in powerBi?'\n",
    "#user_question = 'Tell me about Line Charts'\n",
    "\n",
    "user_question = \"How can I do a pie chart in PowerBI?\"\n",
    "docs = db.similarity_search(user_question, k=10)\n",
    "\n",
    "# print results\n",
    "for doc in docs[0:3]:\n",
    "    print(doc.page_content, '\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_document_prompt(docs):\n",
    "    prompt = '\\n'\n",
    "    for doc in docs:\n",
    "        prompt += '\\nContent:\\n'\n",
    "        prompt += doc.page_content + '\\n\\n'\n",
    "    return prompt\n",
    "\n",
    "print(_get_document_prompt(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Step Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "prompt = f\"\"\"\n",
    "## INTRODUCTION\n",
    "You are a Chatbot designed to help answer technical questions about a software.\n",
    "The user asked: \"{user_question}\"\n",
    "\n",
    "## CONTEXT\n",
    "Technical Documentation for the software:\n",
    "'''\n",
    "{_get_document_prompt(docs)}\n",
    "'''\n",
    "\n",
    "## RESTRICTIONS\n",
    "Refer to the products by their names.\n",
    "Be clear, transparent, and factual: only state what is in the context without providing opinions or subjectivity.\n",
    "Answer the question based solely on the context above; if you do not know the answer, be clear with the user that you do not know.\n",
    "Only respond to questions related to the products, avoiding jokes, offensive remarks, and discussions on religion or sexuality.\n",
    "If the user does not provide sufficient context, do not answer and instead ask for more information on what the user wants to know.\n",
    "\n",
    "## TASK\n",
    "First, answer directly to the user, if possible\n",
    "Second, point the user int he right direction of the documentation\n",
    "Lastly, answer in Markdown format\n",
    "\n",
    "## RESPONSE STRUCTURE:\n",
    "'''\n",
    "# [Answer Title]\n",
    "[answer text]\n",
    "\n",
    "Source:\n",
    "• From pages [...] of the Technical Documentation for *Product1* (link)\n",
    "• From pages [...] of the Technical Documentation for *Product2* (link)\n",
    "'''\n",
    "\n",
    "## CONVERSATION:\n",
    "User: {user_question}\n",
    "Agent:\n",
    "\"\"\"\n",
    "\n",
    "client = OpenAI(api_key = API_KEY)\n",
    "\n",
    "messages = [{'role':'user', 'content':prompt}]\n",
    "model_params = {'model': 'gpt-4o', 'temperature': 0.4, 'max_tokens': 3000}\n",
    "completion = client.chat.completions.create(messages=messages, **model_params, timeout=120)\n",
    "\n",
    "\n",
    "answer = completion.choices[0].message.content\n",
    "model = completion.model\n",
    "\n",
    "print(user_question)\n",
    "print(f'From Model: {model}:\\n')\n",
    "print(answer)\n",
    "print('\\n ------------ \\n')\n",
    "from IPython.display import display, HTML, Markdown\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps of  RAG System:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "\n",
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set your OpenAI API key\n",
    "#API_KEY = \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the document\n",
    "document_dir = \"./\"\n",
    "filename = \"powerbi_book.pdf\"\n",
    "file_path = os.path.join(document_dir, filename)\n",
    "\n",
    "# Use PyPDFLoader to load the PDF\n",
    "pages = PyPDFLoader(file_path).load_and_split()\n",
    "print(f\"Loaded {len(pages)} pages from the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings function using OpenAI's model throught the API\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=API_KEY)\n",
    "print(\"Initialized OpenAI embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings function using SentenceTransformer's model - LOCAL MODEL\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "sentence_transformer_embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "print(\"Initialized SentenceTransformer embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### COMPARE EMBEDDING VECTORS\n",
    "\n",
    "# Sample text\n",
    "sample_text = \"How can I create a pie chart in PowerBI?\"\n",
    "\n",
    "# Generate embeddings\n",
    "openai_vector = openai_embeddings.embed_query(sample_text)\n",
    "sentence_transformer_vector = sentence_transformer_embeddings.embed_query(sample_text)\n",
    "\n",
    "# Display vector dimensions\n",
    "print(f\"OpenAI Embedding Dimension: {len(openai_vector)}\")\n",
    "print(f\"OpenAI Embedding Vector: {openai_vector[0:5]}\")\n",
    "print()\n",
    "print(f\"SentenceTransformer Embedding Dimension: {len(sentence_transformer_vector)}\")\n",
    "print(f\"SentenceTransformer Embedding Vector: {sentence_transformer_vector[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Text Splitters\n",
    "\n",
    "To understand TextSplitting please check this Notebook:\n",
    "https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CharacterTextSplitter\n",
    "char_text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# Split documents\n",
    "docs_char_split = char_text_splitter.split_documents(pages)\n",
    "print(f\"Number of chunks with CharacterTextSplitter: {len(docs_char_split)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RecursiveCharacterTextSplitter: https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# Split documents\n",
    "docs_recursive_split = recursive_text_splitter.split_documents(pages)\n",
    "print(f\"Number of chunks with RecursiveCharacterTextSplitter: {len(docs_recursive_split)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare first chunk\n",
    "print(\"6th chunk using CharacterTextSplitter:\")\n",
    "print(docs_char_split[5].page_content[:500], \"\\n\")\n",
    "\n",
    "print(\"6th chunk using RecursiveCharacterTextSplitter:\")\n",
    "print(docs_recursive_split[5].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Honorable mention - Semantic Chunking: still experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "semantic_chunk_splitter = SemanticChunker(sentence_transformer_embeddings)\n",
    "\n",
    "docs_semantic_split = semantic_chunk_splitter.split_documents(pages)\n",
    "print(f\"Number of chunks with SemanticChunkerr: {len(docs_semantic_split)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![semantic_chunking](./img/semantic_chunking.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100, 110):\n",
    "    print(docs_semantic_split[i].page_content)\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ChromaDB instance - OpenAI\n",
    "db_openai = Chroma.from_documents(docs_recursive_split, \n",
    "                                  openai_embeddings, \n",
    "                                  persist_directory=\"./chroma_db_openai\")\n",
    "\n",
    "print(\"ChromaDB with OpenAI embeddings created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ChromaDB instance - HuggingFace Local Model\n",
    "db_sentence_transformer = Chroma.from_documents(docs_recursive_split, \n",
    "                                                sentence_transformer_embeddings, \n",
    "                                                persist_directory=\"./chroma_db_sentence_transformer\")\n",
    "\n",
    "print(\"ChromaDB with SentenceTransformer embeddings created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval with ChromaDB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrieve Documents Using OpenAI Embeddings\n",
    "# User question\n",
    "user_question = \"How can I create a pie chart in PowerBI?\"\n",
    "\n",
    "# Retrieve documents\n",
    "retrieved_docs_openai = db_openai.similarity_search(user_question, k=5)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 3 documents retrieved using OpenAI embeddings:\\n\")\n",
    "for idx, doc in enumerate(retrieved_docs_openai[:3], 1):\n",
    "    print(f\"Document {idx}:\\n{doc.page_content[:500]}\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Retrieve Documents Using SentenceTransformer Embeddings\n",
    "# Retrieve documents\n",
    "retrieved_docs_sentence = db_sentence_transformer.similarity_search(user_question, k=5)\n",
    "\n",
    "# Display results\n",
    "print(\"Top 3 documents retrieved using SentenceTransformer embeddings:\\n\")\n",
    "for idx, doc in enumerate(retrieved_docs_sentence[:3], 1):\n",
    "    print(f\"Document {idx}:\\n{doc.page_content[:500]}\\n{'-'*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyse Retreival Results:\n",
    "\n",
    "# Function to extract snippets\n",
    "def extract_snippets(docs):\n",
    "    return [doc.page_content[:200] for doc in docs]\n",
    "\n",
    "# Extract snippets\n",
    "snippets_openai = extract_snippets(retrieved_docs_openai)\n",
    "snippets_sentence = extract_snippets(retrieved_docs_sentence)\n",
    "\n",
    "# Display comparison\n",
    "print(\"Comparison of retrieval results:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Result {i+1} with OpenAI embeddings:\\n{snippets_openai[i]}\\n\")\n",
    "    print(f\"Result {i+1} with SentenceTransformer embeddings:\\n{snippets_sentence[i]}\\n\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we:\n",
    "\n",
    "- Explored two embedding models: OpenAI's `text-embedding-ada-002` and SentenceTransformer's `all-MiniLM-L6-v2`.\n",
    "- Compared their embedding dimensions and initialization processes.\n",
    "- Experimented with two text splitting methods: `CharacterTextSplitter` and `RecursiveCharacterTextSplitter`.\n",
    "- Observed the number of chunks produced and the content of the first chunk from each splitter.\n",
    "- Built two separate ChromaDB vector stores using the different embeddings and split documents.\n",
    "- Performed similarity searches to retrieve documents relevant to the user's question.\n",
    "- Compared the retrieval results to analyze which combination provided more relevant information.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- Different embedding models produce vectors of different dimensions, which may impact retrieval performance.\n",
    "- The choice of text splitter affects how the document is chunked and can influence the context preserved in each chunk.\n",
    "- Comparing retrieval results helps in selecting the best combination of embedding models and text splitters for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve the best documents for the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User's question\n",
    "user_question = \"How can I create a pie chart in PowerBI?\"\n",
    "\n",
    "# Retrieve documents\n",
    "# Assume 'retrieved_docs' is the list of documents retrieved from the vector store\n",
    "# For this example, we will use the documents retrieved using OpenAI embeddings\n",
    "# If you used SentenceTransformer embeddings, replace 'db_openai' with 'db_sentence_transformer'\n",
    "retrieved_docs = db_openai.similarity_search(user_question, k=5)\n",
    "\n",
    "# Function to combine documents into a single string\n",
    "def _get_document_prompt(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Prepare the context from the retrieved documents\n",
    "context = _get_document_prompt(retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the prompt\n",
    "prompt = f\"\"\"\n",
    "## INTRODUCTION\n",
    "You are a Chatbot designed to help answer technical questions about a software.\n",
    "The user asked: \"{user_question}\"\n",
    "\n",
    "## CONTEXT\n",
    "Technical Documentation for the software:\n",
    "'''\n",
    "{context}\n",
    "'''\n",
    "\n",
    "## RESTRICTIONS\n",
    "Refer to the products by their names.\n",
    "Be clear, transparent, and factual: only state what is in the context without providing opinions or subjectivity.\n",
    "Answer the question based solely on the context above; if you do not know the answer, be clear with the user that you do not know.\n",
    "Only respond to questions related to the products, avoiding jokes, offensive remarks, and discussions on religion or sexuality.\n",
    "If the user does not provide sufficient context, do not answer and instead ask for more information on what the user wants to know.\n",
    "\n",
    "## TASK\n",
    "First, answer directly to the user, if possible.\n",
    "Second, point the user in the right direction of the documentation.\n",
    "Lastly, answer in Markdown format.\n",
    "\n",
    "## RESPONSE STRUCTURE:\n",
    "'''\n",
    "# [Answer Title]\n",
    "[answer text]\n",
    "\n",
    "Source:\n",
    "• From pages [...] of the Technical Documentation for *Product1* (link)\n",
    "• From pages [...] of the Technical Documentation for *Product2* (link)\n",
    "'''\n",
    "\n",
    "## CONVERSATION:\n",
    "User: {user_question}\n",
    "Agent:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "model_gpt = \"gpt-4o-mini\"\n",
    "\n",
    "# Prepare the messages payload\n",
    "messages = [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "# Set model parameters\n",
    "model_params = {'model': model_gpt, 'temperature': 0.4, 'max_tokens': 3000}\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages = messages,\n",
    "    model = model_gpt,\n",
    ")\n",
    "\n",
    "answer = chat_completion.choices[0].message.content\n",
    "print(answer)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
